{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T00:06:41.333846Z",
     "start_time": "2020-12-17T00:06:40.238883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.patches as patches\n",
    "from scipy.spatial.transform import Rotation\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import scipy.io as sio\n",
    "from utils.renderer import Renderer\n",
    "from utils.image_operations import expand_bbox_rectangle\n",
    "from utils.pose_operations import get_pose\n",
    "from img2pose import img2poseModel\n",
    "from model_loader import load_model\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T00:06:41.340777Z",
     "start_time": "2020-12-17T00:06:41.335946Z"
    }
   },
   "outputs": [],
   "source": [
    "def render_plot(img, poses, bboxes):\n",
    "    (w, h) = img.size\n",
    "    image_intrinsics = np.array([[w + h, 0, w // 2], [0, w + h, h // 2], [0, 0, 1]])\n",
    "    \n",
    "    trans_vertices = renderer.transform_vertices(img, poses)\n",
    "    img = renderer.render(img, trans_vertices, alpha=1)    \n",
    "\n",
    "    plt.figure(figsize=(8, 8))     \n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        plt.gca().add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],linewidth=3,edgecolor='b',facecolor='none'))            \n",
    "    \n",
    "    plt.imshow(img)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the renderer for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T00:06:41.357319Z",
     "start_time": "2020-12-17T00:06:41.342763Z"
    }
   },
   "outputs": [],
   "source": [
    "renderer = Renderer(\n",
    "    vertices_path=\"../../pose_references/vertices_trans.npy\", \n",
    "    triangles_path=\"../../pose_references/triangles.npy\"\n",
    ")\n",
    "\n",
    "threed_points = np.load('../../pose_references/reference_3d_68_points_trans.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights and pose mean and std deviation\n",
    "To test other models, change MODEL_PATH along the the POSE_MEAN and POSE_STDDEV used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T00:06:44.297174Z",
     "start_time": "2020-12-17T00:06:41.359153Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "DEPTH = 18\n",
    "MAX_SIZE = 1400\n",
    "MIN_SIZE = 600\n",
    "\n",
    "POSE_MEAN = \"../../models/WIDER_train_pose_mean_v1.npy\"\n",
    "POSE_STDDEV = \"../../models/WIDER_train_pose_stddev_v1.npy\"\n",
    "MODEL_PATH = \"../../models/img2pose_v1.pth\"\n",
    "\n",
    "pose_mean = np.load(POSE_MEAN)\n",
    "pose_stddev = np.load(POSE_STDDEV)\n",
    "\n",
    "img2pose_model = img2poseModel(\n",
    "    DEPTH, MIN_SIZE, MAX_SIZE, \n",
    "    pose_mean=pose_mean, pose_stddev=pose_stddev,\n",
    "    threed_68_points=threed_points,\n",
    ")\n",
    "load_model(img2pose_model.fpn_model, MODEL_PATH, cpu_mode=str(img2pose_model.device) == \"cpu\", model_only=True)\n",
    "img2pose_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Open the video file\n",
    "\n",
    "video = cv2.VideoCapture('/content/img2pose/data/sample.mp4')\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize frame counter\n",
    "frame_count = 0\n",
    "\n",
    "# Create a directory to store the frames\n",
    "if not os.path.exists('/content/img2pose/results/frames'):\n",
    "    os.makedirs('/content/img2pose/results/frames')\n",
    "\n",
    "# Loop through the frames of the video\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If there are no more frames, break out of the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save the current frame as an image file in the 'frames' folder\n",
    "    cv2.imwrite(f'/content/img2pose/results/frames/frame{frame_count}.jpg', frame)\n",
    "\n",
    "    # Increment the frame counter\n",
    "    frame_count += 1\n",
    "\n",
    "    # Wait for a short time to simulate the video playback speed\n",
    "    cv2.waitKey(int(1000/fps))\n",
    "\n",
    "# Release the video file\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    " \n",
    "\n",
    "# change to a folder with images, or another list containing image paths\n",
    "images_path = \"/content/img2pose/results/frames\"\n",
    "\n",
    "# Create a directory to store the frames\n",
    "if not os.path.exists('/content/img2pose/results/maskedframes'):\n",
    "    os.makedirs('/content/img2pose/results/maskedframes')\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "if os.path.isfile(images_path):\n",
    "    img_paths = pd.read_csv(images_path, delimiter=\" \", header=None)\n",
    "    img_paths = np.asarray(img_paths).squeeze()\n",
    "else:\n",
    "    img_paths = [os.path.join(images_path, img_path) for img_path in os.listdir(images_path) if img_path.endswith(\".jpg\")]\n",
    "\n",
    "for img_path in tqdm(img_paths):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    image_name = os.path.split(img_path)[1]\n",
    "    \n",
    "    (w, h) = img.size\n",
    "    image_intrinsics = np.array([[w + h, 0, w // 2], [0, w + h, h // 2], [0, 0, 1]])\n",
    "            \n",
    "    res = img2pose_model.predict([transform(img)])[0]\n",
    "\n",
    "    all_bboxes = res[\"boxes\"].cpu().numpy().astype('float')\n",
    "\n",
    "    poses = []\n",
    "    bboxes = []\n",
    "    for i in range(len(all_bboxes)):\n",
    "        if res[\"scores\"][i] > threshold:\n",
    "            bbox = all_bboxes[i]\n",
    "            pose_pred = res[\"dofs\"].cpu().numpy()[i].astype('float')\n",
    "            pose_pred = pose_pred.squeeze()\n",
    "\n",
    "            poses.append(pose_pred)  \n",
    "            bboxes.append(bbox)\n",
    "\n",
    "    # Assuming you have already initialized the renderer and loaded the image\n",
    "    trans_vertices = renderer.transform_vertices(img, poses)\n",
    "    img = renderer.render(img, trans_vertices, alpha=1)\n",
    "\n",
    "    # Create a new plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Add bounding boxes to the plot (assuming you have already defined the bboxes)\n",
    "    for bbox in bboxes:\n",
    "        ax.add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                                        linewidth=3, edgecolor='b', facecolor='none'))\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('/content/img2pose/results/maskedframes/'+image_name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the frames\n",
    "frames_dir = '/content/img2pose/results/maskedframes'\n",
    "\n",
    "# Get a list of all the frame filenames\n",
    "frame_files = sorted(os.listdir(frames_dir))\n",
    "\n",
    "# Load the first frame to get the frame size\n",
    "frame = cv2.imread(os.path.join(frames_dir, frame_files[0]))\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Create the video writer object\n",
    "video_writer = cv2.VideoWriter('/content/img2pose/results/masked_video_people.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 25, (width, height))\n",
    "\n",
    "# Loop over all the frames and add them to the video\n",
    "for i in range(1, len(frame_files)):\n",
    "    filename =frames_dir+'/frame'+str(i)+'.jpg'\n",
    "    frame = cv2.imread(filename)\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release the video writer and close the video file\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "336305087387084"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200629-000305",
   "description": "Visualization of smaller model pose and expression qualitative results (trial 4).\nResNet-18 with sum of squared errors weighted equally for both pose and expression.\nFC layers for both pose and expression are fc1 512x512 and fc2 512 x output (output is either 6 or 72).\n",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/valbiero/fbsource/fbcode/bento/kernels/local/deep_3d_face_modeling/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "296232",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "1126967097689153",
   "tags": "",
   "tasks": "",
   "title": "Updated Model Pose and Expression Qualitative Results"
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
